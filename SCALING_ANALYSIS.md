# Quidditch/Diagon Scaling Analysis: Technical Deep Dive

**Date:** 2026-01-26
**Question:** What causes lag on 10M+ docs? Can Quidditch scale to bigger clusters?

## TL;DR

**Short Answer:**
1. Lag on 10M+ docs is caused by **in-memory storage** + **exact algorithms**, not architectural limitations
2. Yes, Quidditch **CAN scale horizontally** - we already have sharding and distributed search!
3. What's missing: **multi-node clustering** (multiple physical machines)

---

## Current Scaling Capabilities âœ…

### What We Already Built (Week 5 Day 5)

**1. Sharding Architecture**
```cpp
ShardManager
â”œâ”€â”€ Consistent hashing (document distribution)
â”œâ”€â”€ Multiple shards per node
â”œâ”€â”€ Shard registration and management
â””â”€â”€ Cluster topology tracking
```

**2. Distributed Search**
```cpp
DistributedSearchCoordinator
â”œâ”€â”€ Parallel query execution across shards
â”œâ”€â”€ Result merging with global ranking
â”œâ”€â”€ Aggregation merging (terms + stats)
â””â”€â”€ Partial failure handling
```

**3. Performance Achieved**
- âœ… 10,000 documents across 4 shards: <10ms query latency
- âœ… Parallel shard execution using `std::async`
- âœ… Score-based global result merging
- âœ… Aggregation merging across shards

### Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             Single Node (Current)               â”‚
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  DistributedSearchCoordinator         â”‚    â”‚
â”‚  â”‚  (Merges results from all shards)     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚           â”‚          â”‚          â”‚              â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â–¼â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚     â”‚ Shard 0  â”‚ â”‚ Shard 1â”‚ â”‚ Shard 2â”‚       â”‚
â”‚     â”‚ 1-2GB    â”‚ â”‚ 1-2GB  â”‚ â”‚ 1-2GB  â”‚       â”‚
â”‚     â”‚ RAM      â”‚ â”‚ RAM    â”‚ â”‚ RAM    â”‚       â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                 â”‚
â”‚  Total: ~64GB RAM server can hold ~30M docs   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**This is "horizontal scaling within a node"** - we can add more shards up to RAM limits.

---

## What Causes 10M+ Doc Lag? ğŸ”

### Root Cause Analysis

**Problem:** Not the distributed architecture (that works!), but **in-memory storage** + **exact algorithms**

### Issue #1: In-Memory Document Storage

**Current Implementation:**
```cpp
class DocumentStore {
private:
    std::unordered_map<std::string, std::shared_ptr<StoredDocument>> documents_;
    // ALL documents in RAM!
};
```

**Memory Calculation:**
```
Single document (typical):
â”œâ”€â”€ JSON data: ~1-5 KB (depending on content)
â”œâ”€â”€ Inverted index entries: ~500 bytes
â”œâ”€â”€ Field lengths, metadata: ~200 bytes
â””â”€â”€ Total per doc: ~2-6 KB average

10 million documents:
â”œâ”€â”€ 10M Ã— 3KB (avg) = 30 GB
â”œâ”€â”€ Inverted index overhead: ~5 GB
â”œâ”€â”€ Internal structures: ~2 GB
â””â”€â”€ Total: ~37 GB RAM minimum

Single node with 64GB RAM:
â”œâ”€â”€ Available: ~50GB for data (after OS, apps)
â”œâ”€â”€ Can handle: ~15M documents comfortably
â”œâ”€â”€ Max: ~20M documents before swapping
â””â”€â”€ 10M+ docs causes memory pressure, slowdowns
```

**The Problem:**
- All documents must fit in RAM
- No disk-backed storage
- No eviction policy
- GC/allocation overhead on large heaps

### Issue #2: Exact Aggregation Algorithms

**Current Implementations:**

**Cardinality (Exact Counting):**
```cpp
std::unordered_set<std::string> uniqueValues;
for (const auto& docId : docIds) {
    // Store every unique value in memory
    uniqueValues.insert(value);
}
// Memory: O(unique_count) - unbounded!
```

**Memory Impact:**
```
10M documents, 1M unique users:
â”œâ”€â”€ Hash set storage: 1M Ã— 32 bytes (avg) = 32 MB
â”œâ”€â”€ Hash table overhead (2x): 64 MB
â””â”€â”€ Total: ~100 MB per cardinality aggregation

If you have 5 unique fields aggregated:
â””â”€â”€ 500 MB just for cardinality!
```

**Percentiles (Full Sort):**
```cpp
std::vector<double> values;
// Collect ALL values
for (const auto& docId : docIds) {
    values.push_back(value);
}
// Sort ALL values
std::sort(values.begin(), values.end());  // O(n log n)
```

**Memory + CPU Impact:**
```
10M documents:
â”œâ”€â”€ Vector storage: 10M Ã— 8 bytes = 80 MB
â”œâ”€â”€ Sort time: ~500ms (10M log 10M comparisons)
â””â”€â”€ Total: Significant latency spike

If query touches all 10M docs:
â””â”€â”€ Percentiles alone: 500ms just to sort
```

### Issue #3: No Result Caching

```cpp
// Every query starts from scratch
SearchResult search(query) {
    // 1. Parse query
    // 2. Execute search
    // 3. Calculate aggregations
    // 4. Return results
    // No caching of intermediate results!
}
```

**Impact:**
- Same query repeated: Same work repeated
- Aggregations recalculated every time
- No query result cache
- No aggregation result cache

### Issue #4: No Incremental Processing

```cpp
// Current: All-or-nothing processing
auto results = coordinator->search(query);
// Must collect ALL results before returning

// What's missing: Streaming
// Cannot yield partial results
// Cannot limit processing mid-flight
```

---

## Can Quidditch Scale to Bigger Clusters? ğŸ“ˆ

### Short Answer: YES, but needs multi-node support

### What We Have âœ…

**1. Intra-Node Horizontal Scaling**
```
Current: Multiple shards on ONE machine
â”œâ”€â”€ Shard 0 (2GB RAM)
â”œâ”€â”€ Shard 1 (2GB RAM)
â”œâ”€â”€ Shard 2 (2GB RAM)
â”œâ”€â”€ ...
â””â”€â”€ Limited by single machine RAM
```

**2. Parallel Processing**
- `std::async` for concurrent shard queries
- Result merging with global ranking
- Aggregation merging

**3. Consistent Hashing**
- Documents distributed evenly across shards
- Hash-based routing (deterministic)
- Shard affinity maintained

### What's Missing âŒ

**Multi-Node Clustering** (Inter-Node Horizontal Scaling)

```
Target: Multiple shards across MULTIPLE machines

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Node 1     â”‚  â”‚   Node 2     â”‚  â”‚   Node 3     â”‚
â”‚  (64GB RAM)  â”‚  â”‚  (64GB RAM)  â”‚  â”‚  (64GB RAM)  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Shard 0, 3  â”‚  â”‚  Shard 1, 4  â”‚  â”‚  Shard 2, 5  â”‚
â”‚  ~20M docs   â”‚  â”‚  ~20M docs   â”‚  â”‚  ~20M docs   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                 â”‚                 â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚  Coordinator    â”‚
                 â”‚  (Query Router) â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total Capacity: 60M documents across 3 nodes
```

**Required Components:**

1. **Network Communication**
   ```cpp
   // Need: RPC/gRPC layer for shard-to-shard communication
   class RemoteShard {
       SearchResult search(query) override {
           // Make network call to remote node
           return grpc_client->Search(query);
       }
   };
   ```

2. **Cluster Membership**
   ```cpp
   class ClusterManager {
       std::vector<NodeInfo> nodes_;

       void addNode(NodeInfo node);
       void removeNode(std::string nodeId);
       void heartbeat();  // Monitor node health
       void rebalance();  // Redistribute shards on node changes
   };
   ```

3. **Distributed Coordinator**
   ```cpp
   class MultiNodeCoordinator {
       // Route queries to remote nodes
       std::vector<SearchResult> scatterGather(
           query,
           std::vector<RemoteNode> nodes
       ) {
           // Send query to all nodes
           // Wait for responses
           // Merge results
       }
   };
   ```

4. **Data Replication** (Optional but important)
   ```cpp
   class ReplicationManager {
       void replicate(docId, data, primaryShard, replicaShards);
       void handleNodeFailure(nodeId);
       void electNewPrimary(shardId);
   };
   ```

5. **Distributed Locking** (For consistency)
   ```cpp
   // Prevent concurrent modifications during rebalancing
   class DistributedLock {
       bool acquire(resourceId, timeout);
       void release(resourceId);
   };
   ```

---

## Scaling Strategies: Path Forward ğŸš€

### Strategy 1: Optimize Current Implementation (Quick Wins)

**1.1 Add Result Caching**
```cpp
class QueryCache {
    LRUCache<QueryHash, SearchResult> cache_;

    SearchResult* get(const std::string& query);
    void put(const std::string& query, SearchResult result);
};
```
**Impact:** 10-100x speedup for repeated queries

**1.2 Implement Approximate Algorithms**
```cpp
// Replace exact cardinality with HyperLogLog
class HyperLogLog {
    uint8_t registers_[16384];  // Fixed 16KB memory

    void add(const std::string& value);
    int64_t estimate();  // ~2% error, constant memory
};

// Replace sort-based percentiles with TDigest
class TDigest {
    std::vector<Centroid> centroids_;  // ~100 centroids

    void add(double value);
    double quantile(double q);  // Streaming, ~1KB memory
};
```
**Impact:** Constant memory usage, handles billions of values

**1.3 Add Disk-Backed Storage**
```cpp
class HybridDocumentStore {
    // Hot data in RAM
    LRUCache<std::string, Document> hotCache_;

    // Cold data on disk (mmap, RocksDB, etc.)
    RocksDB* coldStorage_;

    Document get(std::string docId) {
        if (auto doc = hotCache_.get(docId)) return doc;
        return coldStorage_->get(docId);  // Page fault, load from disk
    }
};
```
**Impact:** Scale to 100M+ docs on single node

**1.4 Implement Early Termination**
```cpp
SearchResult search(query, topK) {
    std::priority_queue<Document> topResults;

    for (const auto& doc : documents_) {
        if (topResults.size() >= topK * 10) {
            // Have enough results, can terminate early
            if (doc.score < topResults.top().score * 0.5) {
                break;  // Remaining docs unlikely to be relevant
            }
        }
        // ...
    }
}
```
**Impact:** 2-5x speedup on large result sets

### Strategy 2: Multi-Node Clustering (Long-term)

**Phase 1: Network Layer (2-3 weeks)**
```cpp
// Add gRPC for inter-node communication
service DiagonService {
    rpc Search(SearchRequest) returns (SearchResponse);
    rpc IndexDocument(Document) returns (Status);
    rpc GetDocument(GetRequest) returns (Document);
}
```

**Phase 2: Cluster Management (2-3 weeks)**
```cpp
// Add cluster membership using Raft or gossip protocol
class ClusterManager {
    void join(nodeAddress);
    void heartbeat();
    void handleNodeFailure();
    std::vector<NodeInfo> getActiveNodes();
};
```

**Phase 3: Distributed Coordinator (1-2 weeks)**
```cpp
// Extend existing coordinator for multi-node
class MultiNodeCoordinator : public DistributedSearchCoordinator {
    // Override to support remote shards
    SearchResult search(query) override {
        // Query both local and remote shards
    }
};
```

**Phase 4: Replication (2-3 weeks)**
```cpp
// Add replication for fault tolerance
class ReplicationManager {
    void replicateDocument(docId, data, replicas);
    void handleFailover();
};
```

**Total Effort:** 8-12 weeks for full multi-node clustering

### Strategy 3: Hybrid Approach (Recommended)

**Short-term (1-2 weeks):**
1. âœ… Add query result caching (1 day)
2. âœ… Implement HyperLogLog for cardinality (2 days)
3. âœ… Implement TDigest for percentiles (2 days)
4. âœ… Add early termination optimization (1 day)

**Medium-term (4-6 weeks):**
5. Add disk-backed storage layer (2 weeks)
6. Implement hot/cold data tiering (1 week)
7. Add incremental aggregations (1 week)

**Long-term (8-12 weeks):**
8. Build network/RPC layer
9. Implement cluster management
10. Add replication

---

## Comparison: Current vs Optimized vs Multi-Node

| Aspect | Current | After Optimizations | After Multi-Node |
|--------|---------|-------------------|------------------|
| **Max Docs (Single Node)** | 15M | 100M | N/A |
| **Max Docs (Cluster)** | N/A | N/A | Billions |
| **Query Latency (10M docs)** | 500ms+ | 50-100ms | 20-50ms |
| **Memory Usage** | O(n) | O(log n) | Distributed |
| **Cardinality (10M unique)** | 500MB | 16KB | 16KB per node |
| **Percentiles (10M values)** | 80MB + sort | ~1KB stream | ~1KB per node |
| **Repeated Queries** | Same cost | ~0ms (cached) | ~0ms (cached) |
| **Horizontal Scaling** | Within node | Within node | Across nodes âœ… |
| **Fault Tolerance** | None | None | With replication |
| **Dev Effort** | âœ… Done | 2-3 weeks | 10-14 weeks |

---

## OpenSearch Comparison Clarified

### What I Said vs Reality

**In the comparison doc, I said:**
> "Diagon: Vertical (RAM) scaling"
> "OpenSearch: Horizontal (nodes) scaling"

**More Accurate Statement:**
- **Diagon Current:** Horizontal within a node (multiple shards), limited by node RAM
- **Diagon After Optimization:** Horizontal within a node, handles 100M+ docs
- **Diagon After Multi-Node:** True horizontal scaling across nodes, unlimited scale
- **OpenSearch:** True horizontal scaling, production-ready

### OpenSearch's Advantages Remain:

**1. Mature Multi-Node Clustering**
- 15+ years of production hardening
- Automatic shard rebalancing
- Split-brain detection
- Zero-downtime rolling upgrades

**2. Disk-Backed Storage**
- Lucene segment files on disk
- Memory-mapped file I/O
- Page cache optimization
- Handles TB of data per node

**3. Approximate Algorithms**
- HyperLogLog for cardinality
- TDigest for percentiles
- Adaptive interval for date histogram
- Memory-efficient by default

**4. Advanced Caching**
- Query result cache
- Aggregation result cache
- Filter cache
- Field data cache
- Request cache

**5. Circuit Breakers**
- Memory circuit breaker
- Field data circuit breaker
- Request circuit breaker
- Prevents OOM crashes

---

## Concrete Example: 50M Document Deployment

### Scenario: E-commerce product search (50M products)

**Current Diagon (Single Node with 128GB RAM):**
```
Shard Layout:
â”œâ”€â”€ Shard 0: 12.5M docs (30GB RAM)
â”œâ”€â”€ Shard 1: 12.5M docs (30GB RAM)
â”œâ”€â”€ Shard 2: 12.5M docs (30GB RAM)
â””â”€â”€ Shard 3: 12.5M docs (30GB RAM)
Total: 50M docs in 120GB RAM

Problem: Right at memory limit, no headroom!
```

**Optimized Diagon (Single Node, Disk-backed):**
```
Hot Data: 5M most-accessed products in RAM (12GB)
Cold Data: 45M products on SSD (180GB disk)
Total: 50M docs, 12GB RAM, 180GB SSD

Query Performance:
â”œâ”€â”€ Hot data hit: 10ms
â”œâ”€â”€ Cold data hit: 50ms (disk read)
â””â”€â”€ Average: 20ms (75% hot rate)

Works! But single point of failure.
```

**Multi-Node Diagon (4 nodes, 32GB RAM each):**
```
Node 1: Shards 0, 4, 8   (12M docs, 28GB RAM)
Node 2: Shards 1, 5, 9   (12M docs, 28GB RAM)
Node 3: Shards 2, 6, 10  (12M docs, 28GB RAM)
Node 4: Shards 3, 7, 11  (14M docs, 28GB RAM)

Total: 50M docs across 4 nodes
Fault Tolerance: With 2x replication
Query Latency: 15-25ms (parallel across nodes)

Scales to 200M+ docs by adding more nodes!
```

**OpenSearch (3 nodes, 64GB RAM each):**
```
Node 1: Shards 0, 3 (primary), Shards 1, 4 (replica)
Node 2: Shards 1, 4 (primary), Shards 2, 5 (replica)
Node 3: Shards 2, 5 (primary), Shards 0, 3 (replica)

Total: 50M docs, 2x replication, automatic failover
Query Latency: 10-20ms
Disk Usage: ~200GB per node (compressed)
Memory: ~40GB per node (includes JVM heap + page cache)

Mature, production-ready, auto-scaling!
```

---

## Summary: Key Takeaways

### 1. What Causes 10M+ Lag?

**Primary Causes:**
- âœ… In-memory document storage (not scalable beyond RAM)
- âœ… Exact aggregation algorithms (O(n) memory)
- âœ… No result caching
- âœ… No approximate algorithms
- âŒ NOT the distributed architecture (that works!)

**Solutions:**
- Add HyperLogLog + TDigest (2-3 days work)
- Add query caching (1 day work)
- Add disk-backed storage (2 weeks work)

### 2. Can Quidditch Scale Horizontally?

**Yes! We already have:**
- âœ… Sharding with consistent hashing
- âœ… Distributed search coordinator
- âœ… Parallel query execution
- âœ… Result merging across shards
- âœ… Horizontal scaling **within a node**

**What we need for "true" horizontal scaling:**
- âŒ Multi-node clustering (8-12 weeks)
- âŒ Network/RPC layer
- âŒ Cluster management
- âŒ Replication

### 3. Immediate Path Forward (Recommended)

**Week 1: Quick Wins**
- [ ] Add HyperLogLog cardinality (replaces exact counting)
- [ ] Add TDigest percentiles (replaces full sort)
- [ ] Add LRU query result cache
- [ ] Add early termination for top-K queries

**Expected Impact:**
- 10M docs: 500ms â†’ 50ms (10x improvement)
- Cardinality memory: 500MB â†’ 16KB (30,000x reduction)
- Repeated queries: Full cost â†’ ~0ms (cache hit)

**Week 2-4: Disk-backed Storage**
- [ ] Add RocksDB integration
- [ ] Implement hot/cold tiering
- [ ] Add eviction policies

**Expected Impact:**
- Max docs: 15M â†’ 100M+ per node
- Query latency: Slight increase on cold data (~50ms)

**Week 5-16: Multi-Node Support (if needed)**
- [ ] Network layer (gRPC)
- [ ] Cluster management
- [ ] Replication
- [ ] Distributed coordinator

**Expected Impact:**
- Unlimited scaling (add more nodes)
- Fault tolerance
- True distributed system

### 4. Corrected Comparison

| Capability | Current Diagon | After Optimization | After Multi-Node | OpenSearch |
|------------|---------------|-------------------|------------------|------------|
| **Sharding** | âœ… Yes | âœ… Yes | âœ… Yes | âœ… Yes |
| **Intra-node horizontal** | âœ… Yes | âœ… Yes | âœ… Yes | âœ… Yes |
| **Inter-node horizontal** | âŒ No | âŒ No | âœ… Yes | âœ… Yes |
| **Max docs (single)** | 15M | 100M | 100M | 100M |
| **Max docs (cluster)** | N/A | N/A | Unlimited | Unlimited |
| **Approximate algorithms** | âŒ No | âœ… Yes | âœ… Yes | âœ… Yes |
| **Disk-backed** | âŒ No | âœ… Yes | âœ… Yes | âœ… Yes |
| **Query caching** | âŒ No | âœ… Yes | âœ… Yes | âœ… Yes |

---

## Conclusion

**The good news:**
1. âœ… We already have solid distributed search within a node
2. âœ… The architecture is sound and ready for optimization
3. âœ… Quick wins (1-2 weeks) can handle 100M docs on single node
4. âœ… Path to true multi-node scaling is clear (8-12 weeks)

**The 10M+ lag is not a fundamental limitation** - it's a trade-off we made (exact algorithms, in-memory storage) that can be addressed with well-known optimizations (HyperLogLog, TDigest, disk backing).

**Horizontal scaling:** We CAN scale horizontally! We just need to extend from "multiple shards on one node" to "multiple shards across multiple nodes" - which is a matter of engineering effort, not architectural redesign.
