# Diagon Analyzer Framework Design
## Text Analysis with Chinese Support (Jieba)
## Date: January 27, 2026

---

## Executive Summary

**Goal**: Implement a comprehensive text analysis framework in Diagon C++ core with:
- Standard English analyzers (Standard, Whitespace, Simple, Keyword)
- Chinese text analysis via Jieba word segmentation
- Extensible architecture for custom analyzers
- C API exposure for Go integration

**Estimated Time**: 10-12 days (80-96 hours)
**Priority**: HIGH (foundation for search quality)
**Dependencies**: Jieba C++ library

---

## Architecture

### Core Components

```cpp
// Base interfaces
class Token {
    std::string text;
    int position;
    int startOffset;
    int endOffset;
    std::string type;
};

class Tokenizer {
    virtual std::vector<Token> tokenize(const std::string& text) = 0;
};

class TokenFilter {
    virtual std::vector<Token> filter(const std::vector<Token>& tokens) = 0;
};

class Analyzer {
    virtual std::vector<Token> analyze(const std::string& text) = 0;
    virtual std::string name() const = 0;
};
```

### Component Hierarchy

```
Analyzer (orchestrates tokenization and filtering)
    ├── Tokenizer (breaks text into tokens)
    │   ├── StandardTokenizer (Unicode-aware, handles punctuation)
    │   ├── WhitespaceTokenizer (splits on whitespace)
    │   ├── KeywordTokenizer (no tokenization)
    │   ├── LetterTokenizer (splits on non-letters)
    │   └── JiebaTokenizer (Chinese word segmentation)
    │
    └── TokenFilters[] (process token stream)
        ├── LowercaseFilter
        ├── StopFilter (remove stop words)
        ├── ASCIIFoldingFilter (café → cafe)
        ├── SynonymFilter (laptop → [laptop, notebook])
        ├── EdgeNGramFilter (for autocomplete)
        └── StemmerFilter (running → run)
```

---

## Implementation Phases

### Phase 1: Core Framework (2 days)

**Files to create**:
- `pkg/data/diagon/upstream/src/core/include/analysis/Token.h`
- `pkg/data/diagon/upstream/src/core/include/analysis/Tokenizer.h`
- `pkg/data/diagon/upstream/src/core/include/analysis/TokenFilter.h`
- `pkg/data/diagon/upstream/src/core/include/analysis/Analyzer.h`
- `pkg/data/diagon/upstream/src/core/src/analysis/Token.cpp`

#### Token.h

```cpp
#pragma once
#include <string>

namespace diagon {
namespace analysis {

class Token {
public:
    Token(const std::string& text, int position, int startOffset, int endOffset);

    const std::string& getText() const { return text_; }
    int getPosition() const { return position_; }
    int getStartOffset() const { return startOffset_; }
    int getEndOffset() const { return endOffset_; }
    const std::string& getType() const { return type_; }

    void setText(const std::string& text) { text_ = text; }
    void setType(const std::string& type) { type_ = type; }

private:
    std::string text_;
    int position_;
    int startOffset_;
    int endOffset_;
    std::string type_;
};

} // namespace analysis
} // namespace diagon
```

#### Tokenizer.h

```cpp
#pragma once
#include "Token.h"
#include <vector>
#include <string>
#include <memory>

namespace diagon {
namespace analysis {

class Tokenizer {
public:
    virtual ~Tokenizer() = default;

    // Main tokenization method
    virtual std::vector<Token> tokenize(const std::string& text) = 0;

    // Optional: reset for reuse
    virtual void reset() {}

    // Tokenizer name for debugging
    virtual std::string name() const = 0;
};

} // namespace analysis
} // namespace diagon
```

#### TokenFilter.h

```cpp
#pragma once
#include "Token.h"
#include <vector>
#include <memory>

namespace diagon {
namespace analysis {

class TokenFilter {
public:
    virtual ~TokenFilter() = default;

    // Process token stream
    virtual std::vector<Token> filter(const std::vector<Token>& tokens) = 0;

    // Filter name
    virtual std::string name() const = 0;
};

} // namespace analysis
} // namespace diagon
```

#### Analyzer.h

```cpp
#pragma once
#include "Tokenizer.h"
#include "TokenFilter.h"
#include <vector>
#include <memory>

namespace diagon {
namespace analysis {

class Analyzer {
public:
    virtual ~Analyzer() = default;

    // Analyze text: tokenize + filter
    virtual std::vector<Token> analyze(const std::string& text) = 0;

    // Analyzer name
    virtual std::string name() const = 0;

    // Get component info
    virtual std::string getTokenizerName() const = 0;
    virtual std::vector<std::string> getFilterNames() const = 0;
};

// Standard analyzer implementation
class StandardAnalyzer : public Analyzer {
public:
    StandardAnalyzer(std::unique_ptr<Tokenizer> tokenizer,
                     std::vector<std::unique_ptr<TokenFilter>> filters);

    std::vector<Token> analyze(const std::string& text) override;
    std::string name() const override { return "standard"; }
    std::string getTokenizerName() const override;
    std::vector<std::string> getFilterNames() const override;

private:
    std::unique_ptr<Tokenizer> tokenizer_;
    std::vector<std::unique_ptr<TokenFilter>> filters_;
};

} // namespace analysis
} // namespace diagon
```

**Tests**:
- Token construction and accessors
- Interface contract tests

---

### Phase 2: Standard Tokenizers (2 days)

#### 2.1 WhitespaceTokenizer (Simplest)

**File**: `pkg/data/diagon/upstream/src/core/src/analysis/WhitespaceTokenizer.cpp`

```cpp
#include "analysis/WhitespaceTokenizer.h"
#include <cctype>

namespace diagon {
namespace analysis {

std::vector<Token> WhitespaceTokenizer::tokenize(const std::string& text) {
    std::vector<Token> tokens;
    int position = 0;
    int start = 0;
    bool inToken = false;

    for (size_t i = 0; i < text.length(); i++) {
        if (std::isspace(text[i])) {
            if (inToken) {
                // End of token
                tokens.emplace_back(
                    text.substr(start, i - start),
                    position++,
                    start,
                    i
                );
                inToken = false;
            }
        } else {
            if (!inToken) {
                // Start of token
                start = i;
                inToken = true;
            }
        }
    }

    // Handle last token
    if (inToken) {
        tokens.emplace_back(
            text.substr(start),
            position,
            start,
            text.length()
        );
    }

    return tokens;
}

} // namespace analysis
} // namespace diagon
```

**Test cases**:
- Simple text: "hello world" → ["hello", "world"]
- Multiple spaces: "hello  world" → ["hello", "world"]
- Leading/trailing spaces
- Empty string
- Chinese text: "你好 世界" → ["你好", "世界"]

#### 2.2 KeywordTokenizer

```cpp
std::vector<Token> KeywordTokenizer::tokenize(const std::string& text) {
    std::vector<Token> tokens;
    if (!text.empty()) {
        tokens.emplace_back(text, 0, 0, text.length());
        tokens[0].setType("keyword");
    }
    return tokens;
}
```

#### 2.3 StandardTokenizer (Unicode-aware)

**Dependencies**: ICU library for Unicode text segmentation

```cpp
#include <unicode/uchar.h>
#include <unicode/brkiter.h>

std::vector<Token> StandardTokenizer::tokenize(const std::string& text) {
    std::vector<Token> tokens;

    // Convert to UTF-16 for ICU
    icu::UnicodeString utext(text.c_str(), "UTF-8");

    // Create word break iterator
    UErrorCode status = U_ZERO_ERROR;
    std::unique_ptr<icu::BreakIterator> bi(
        icu::BreakIterator::createWordInstance(icu::Locale::getUS(), status)
    );
    bi->setText(utext);

    int position = 0;
    int32_t start = bi->first();
    for (int32_t end = bi->next(); end != icu::BreakIterator::DONE;
         start = end, end = bi->next()) {

        icu::UnicodeString word;
        utext.extractBetween(start, end, word);

        // Skip pure whitespace and punctuation
        if (isSignificantToken(word)) {
            std::string utf8;
            word.toUTF8String(utf8);
            tokens.emplace_back(utf8, position++, start, end);
        }
    }

    return tokens;
}
```

**Test cases**:
- English: "Hello, world!" → ["Hello", "world"]
- Unicode: "café résumé" → ["café", "résumé"]
- Mixed: "hello世界test" → ["hello", "世界", "test"]
- Punctuation handling
- Numbers: "test123" → ["test123"] or ["test", "123"]?

---

### Phase 3: Jieba Chinese Tokenizer (3 days)

#### 3.1 Integrate Jieba C++ Library

**Library**: cppjieba (https://github.com/yanyiwu/cppjieba)

**Build integration**:

```cmake
# pkg/data/diagon/upstream/CMakeLists.txt

# Download cppjieba
include(FetchContent)
FetchContent_Declare(
    cppjieba
    GIT_REPOSITORY https://github.com/yanyiwu/cppjieba.git
    GIT_TAG v5.0.3
)
FetchContent_MakeAvailable(cppjieba)

# Link to diagon_core
target_link_libraries(diagon_core PRIVATE cppjieba)
```

#### 3.2 JiebaTokenizer Implementation

**File**: `pkg/data/diagon/upstream/src/core/src/analysis/JiebaTokenizer.cpp`

```cpp
#include "analysis/JiebaTokenizer.h"
#include "cppjieba/Jieba.hpp"
#include <memory>

namespace diagon {
namespace analysis {

class JiebaTokenizer::Impl {
public:
    Impl(const std::string& dictPath,
         const std::string& hmmPath,
         const std::string& userDictPath)
        : jieba_(dictPath, hmmPath, userDictPath) {}

    cppjieba::Jieba jieba_;
};

JiebaTokenizer::JiebaTokenizer(const std::string& dictPath,
                               const std::string& hmmPath,
                               const std::string& userDictPath,
                               Mode mode)
    : impl_(std::make_unique<Impl>(dictPath, hmmPath, userDictPath))
    , mode_(mode) {}

JiebaTokenizer::~JiebaTokenizer() = default;

std::vector<Token> JiebaTokenizer::tokenize(const std::string& text) {
    std::vector<std::string> words;

    switch (mode_) {
        case Mode::MP:  // Maximum Probability
            impl_->jieba_.Cut(text, words, true);
            break;
        case Mode::HMM:  // Hidden Markov Model
            impl_->jieba_.Cut(text, words, false);
            break;
        case Mode::MIX:  // Mix mode (default)
            impl_->jieba_.Cut(text, words);
            break;
        case Mode::FULL:  // Full mode
            impl_->jieba_.CutAll(text, words);
            break;
        case Mode::SEARCH:  // Search engine mode
            impl_->jieba_.CutForSearch(text, words);
            break;
    }

    // Convert to Token objects
    std::vector<Token> tokens;
    int position = 0;
    size_t offset = 0;

    for (const auto& word : words) {
        // Calculate byte offset in UTF-8
        size_t wordLen = word.length();
        tokens.emplace_back(word, position++, offset, offset + wordLen);
        offset += wordLen;
    }

    return tokens;
}

std::string JiebaTokenizer::name() const {
    return "jieba_chinese";
}

} // namespace analysis
} // namespace diagon
```

**Dictionary files** (bundled with Jieba):
- `jieba.dict.utf8` - Main dictionary (~400KB)
- `hmm_model.utf8` - HMM model
- `user.dict.utf8` - User dictionary (customizable)
- `idf.utf8` - IDF weights for keyword extraction
- `stop_words.utf8` - Chinese stop words

**Installation in Diagon**:
```bash
# Download Jieba dictionaries
mkdir -p pkg/data/diagon/upstream/dicts/jieba
cd pkg/data/diagon/upstream/dicts/jieba
wget https://raw.githubusercontent.com/fxsjy/jieba/master/extra_dict/jieba.dict.utf8
wget https://raw.githubusercontent.com/fxsjy/jieba/master/extra_dict/hmm_model.utf8
wget https://raw.githubusercontent.com/fxsjy/jieba/master/extra_dict/user.dict.utf8
wget https://raw.githubusercontent.com/fxsjy/jieba/master/extra_dict/idf.utf8
wget https://raw.githubusercontent.com/fxsjy/jieba/master/extra_dict/stop_words.utf8
```

#### 3.3 Jieba Modes

**Mode::MP (Maximum Probability)**:
- Default mode
- Uses dynamic programming to find the most probable segmentation
- Example: "我来到北京清华大学" → ["我", "来到", "北京", "清华大学"]

**Mode::FULL**:
- Enumerates all possible words in the sentence
- Useful for keyword extraction
- Example: "我来到北京清华大学" → ["我", "来到", "北京", "清华", "清华大学", "华大", "大学"]

**Mode::SEARCH**:
- Optimized for search engines
- Further splits long words
- Example: "我来到北京清华大学" → ["我", "来到", "北京", "清华", "华大", "大学", "清华大学"]

**Test cases**:
- Simple: "我爱北京天安门" → ["我", "爱", "北京", "天安门"]
- Ambiguous: "南京市长江大桥" → ["南京市", "长江大桥"] or ["南京", "市长", "江大桥"]?
- English mixed: "我用iPhone" → ["我", "用", "iPhone"]
- Numbers: "我有100元" → ["我", "有", "100", "元"]
- Punctuation: "你好，世界！" → ["你好", "世界"]

---

### Phase 4: Token Filters (2 days)

#### 4.1 LowercaseFilter

```cpp
std::vector<Token> LowercaseFilter::filter(const std::vector<Token>& tokens) {
    std::vector<Token> result;
    result.reserve(tokens.size());

    for (const auto& token : tokens) {
        Token lowercased = token;
        std::string lower = token.getText();

        // Use ICU for proper Unicode lowercasing
        icu::UnicodeString ustr(lower.c_str(), "UTF-8");
        ustr.toLower();
        ustr.toUTF8String(lower);

        lowercased.setText(lower);
        result.push_back(std::move(lowercased));
    }

    return result;
}
```

#### 4.2 StopFilter

```cpp
class StopFilter : public TokenFilter {
public:
    StopFilter(const std::unordered_set<std::string>& stopWords)
        : stopWords_(stopWords) {}

    std::vector<Token> filter(const std::vector<Token>& tokens) override {
        std::vector<Token> result;

        for (const auto& token : tokens) {
            if (stopWords_.find(token.getText()) == stopWords_.end()) {
                result.push_back(token);
            }
        }

        return result;
    }

    std::string name() const override { return "stop"; }

private:
    std::unordered_set<std::string> stopWords_;
};

// English stop words
static const std::unordered_set<std::string> ENGLISH_STOP_WORDS = {
    "a", "an", "and", "are", "as", "at", "be", "but", "by",
    "for", "if", "in", "into", "is", "it", "no", "not", "of",
    "on", "or", "such", "that", "the", "their", "then", "there",
    "these", "they", "this", "to", "was", "will", "with"
};

// Chinese stop words
static const std::unordered_set<std::string> CHINESE_STOP_WORDS = {
    "的", "了", "在", "是", "我", "有", "和", "就", "不", "人",
    "都", "一", "一个", "上", "也", "很", "到", "说", "要", "去"
};
```

#### 4.3 ASCIIFoldingFilter

```cpp
std::vector<Token> ASCIIFoldingFilter::filter(const std::vector<Token>& tokens) {
    std::vector<Token> result;
    result.reserve(tokens.size());

    for (const auto& token : tokens) {
        Token folded = token;

        // Use ICU transliteration
        icu::UnicodeString ustr(token.getText().c_str(), "UTF-8");

        UErrorCode status = U_ZERO_ERROR;
        std::unique_ptr<icu::Transliterator> trans(
            icu::Transliterator::createInstance(
                "NFD; [:Nonspacing Mark:] Remove; NFC",
                UTRANS_FORWARD,
                status
            )
        );

        trans->transliterate(ustr);

        std::string folded_text;
        ustr.toUTF8String(folded_text);
        folded.setText(folded_text);

        result.push_back(std::move(folded));
    }

    return result;
}

// Examples:
// café → cafe
// naïve → naive
// résumé → resume
// Ångström → Angstrom
```

#### 4.4 SynonymFilter

```cpp
class SynonymFilter : public TokenFilter {
public:
    struct SynonymRule {
        std::string word;
        std::vector<std::string> synonyms;
    };

    SynonymFilter(const std::vector<SynonymRule>& rules) {
        for (const auto& rule : rules) {
            synonymMap_[rule.word] = rule.synonyms;
        }
    }

    std::vector<Token> filter(const std::vector<Token>& tokens) override {
        std::vector<Token> result;

        for (const auto& token : tokens) {
            result.push_back(token);

            // Check for synonyms
            auto it = synonymMap_.find(token.getText());
            if (it != synonymMap_.end()) {
                int position = token.getPosition();
                for (const auto& synonym : it->second) {
                    Token synToken = token;
                    synToken.setText(synonym);
                    synToken.setType("synonym");
                    // Same position = synonym
                    result.push_back(synToken);
                }
            }
        }

        return result;
    }

    std::string name() const override { return "synonym"; }

private:
    std::unordered_map<std::string, std::vector<std::string>> synonymMap_;
};

// Example synonym rules:
// laptop → [notebook, computer]
// phone → [mobile, smartphone, cell]
// 笔记本 → [电脑, 计算机]
```

---

### Phase 5: Built-in Analyzers (1 day)

#### 5.1 StandardAnalyzer

```cpp
class StandardAnalyzer : public Analyzer {
public:
    StandardAnalyzer() {
        tokenizer_ = std::make_unique<StandardTokenizer>();
        filters_.push_back(std::make_unique<LowercaseFilter>());
        filters_.push_back(std::make_unique<StopFilter>(ENGLISH_STOP_WORDS));
    }

    std::vector<Token> analyze(const std::string& text) override {
        auto tokens = tokenizer_->tokenize(text);
        for (const auto& filter : filters_) {
            tokens = filter->filter(tokens);
        }
        return tokens;
    }

    std::string name() const override { return "standard"; }
};

// Example:
// Input: "The Quick Brown Fox Jumps!"
// Output: ["quick", "brown", "fox", "jumps"]
```

#### 5.2 ChineseAnalyzer

```cpp
class ChineseAnalyzer : public Analyzer {
public:
    ChineseAnalyzer(const std::string& dictPath) {
        tokenizer_ = std::make_unique<JiebaTokenizer>(
            dictPath + "/jieba.dict.utf8",
            dictPath + "/hmm_model.utf8",
            dictPath + "/user.dict.utf8",
            JiebaTokenizer::Mode::SEARCH
        );
        filters_.push_back(std::make_unique<StopFilter>(CHINESE_STOP_WORDS));
    }

    std::vector<Token> analyze(const std::string& text) override {
        auto tokens = tokenizer_->tokenize(text);
        for (const auto& filter : filters_) {
            tokens = filter->filter(tokens);
        }
        return tokens;
    }

    std::string name() const override { return "chinese"; }
};

// Example:
// Input: "我爱北京天安门"
// Output: ["我", "爱", "北京", "天安门"]
```

#### 5.3 MultilingualAnalyzer

```cpp
class MultilingualAnalyzer : public Analyzer {
public:
    MultilingualAnalyzer(const std::string& dictPath) {
        // Use StandardTokenizer with Jieba fallback
        tokenizer_ = std::make_unique<SmartTokenizer>(dictPath);
        filters_.push_back(std::make_unique<LowercaseFilter>());
        filters_.push_back(std::make_unique<StopFilter>(COMBINED_STOP_WORDS));
    }

    std::vector<Token> analyze(const std::string& text) override;
    std::string name() const override { return "multilingual"; }
};

// SmartTokenizer: detects language and uses appropriate tokenizer
// English/Latin → StandardTokenizer
// Chinese → JiebaTokenizer
// Mixed → Both
```

---

### Phase 6: C API Exposure (1 day)

**File**: `pkg/data/diagon/c_api_src/diagon_c_api.h`

```c
// Analyzer creation
typedef void* DiagonAnalyzer;

// Create standard analyzer
DiagonAnalyzer diagon_create_standard_analyzer(void);

// Create Chinese analyzer
DiagonAnalyzer diagon_create_chinese_analyzer(const char* dict_path);

// Create custom analyzer
DiagonAnalyzer diagon_create_custom_analyzer(
    const char* tokenizer_type,
    const char** filter_types,
    int num_filters
);

// Destroy analyzer
void diagon_destroy_analyzer(DiagonAnalyzer analyzer);

// Analyze text (for testing)
typedef struct {
    char** tokens;
    int* positions;
    int count;
} DiagonTokens;

DiagonTokens* diagon_analyze_text(
    DiagonAnalyzer analyzer,
    const char* text
);

void diagon_free_tokens(DiagonTokens* tokens);

// Configure analyzer for index field
int diagon_set_field_analyzer(
    DiagonIndexWriter writer,
    const char* field_name,
    DiagonAnalyzer analyzer
);

// Set default analyzer for index
int diagon_set_default_analyzer(
    DiagonIndexWriter writer,
    DiagonAnalyzer analyzer
);
```

**Implementation**: `pkg/data/diagon/c_api_src/diagon_c_api.cpp`

```cpp
extern "C" {

DiagonAnalyzer diagon_create_standard_analyzer() {
    try {
        auto analyzer = std::make_shared<diagon::analysis::StandardAnalyzer>();
        return new std::shared_ptr<diagon::analysis::Analyzer>(analyzer);
    } catch (...) {
        return nullptr;
    }
}

DiagonAnalyzer diagon_create_chinese_analyzer(const char* dict_path) {
    try {
        auto analyzer = std::make_shared<diagon::analysis::ChineseAnalyzer>(dict_path);
        return new std::shared_ptr<diagon::analysis::Analyzer>(analyzer);
    } catch (...) {
        return nullptr;
    }
}

DiagonTokens* diagon_analyze_text(DiagonAnalyzer analyzer, const char* text) {
    try {
        auto analyzer_ptr = static_cast<std::shared_ptr<diagon::analysis::Analyzer>*>(analyzer);
        auto tokens = (*analyzer_ptr)->analyze(text);

        DiagonTokens* result = new DiagonTokens();
        result->count = tokens.size();
        result->tokens = new char*[result->count];
        result->positions = new int[result->count];

        for (size_t i = 0; i < tokens.size(); i++) {
            result->tokens[i] = strdup(tokens[i].getText().c_str());
            result->positions[i] = tokens[i].getPosition();
        }

        return result;
    } catch (...) {
        return nullptr;
    }
}

} // extern "C"
```

---

### Phase 7: Go Integration (2 days)

**File**: `pkg/data/diagon/analyzer.go`

```go
package diagon

/*
#include "c_api_src/diagon_c_api.h"
#include <stdlib.h>
*/
import "C"
import (
    "fmt"
    "unsafe"
)

// Analyzer represents a text analyzer
type Analyzer struct {
    ptr C.DiagonAnalyzer
}

// AnalyzerType represents built-in analyzer types
type AnalyzerType string

const (
    AnalyzerTypeStandard    AnalyzerType = "standard"
    AnalyzerTypeSimple      AnalyzerType = "simple"
    AnalyzerTypeWhitespace  AnalyzerType = "whitespace"
    AnalyzerTypeKeyword     AnalyzerType = "keyword"
    AnalyzerTypeChinese     AnalyzerType = "chinese"
    AnalyzerTypeMultilingual AnalyzerType = "multilingual"
)

// NewStandardAnalyzer creates a standard analyzer
func NewStandardAnalyzer() (*Analyzer, error) {
    ptr := C.diagon_create_standard_analyzer()
    if ptr == nil {
        return nil, fmt.Errorf("failed to create standard analyzer")
    }
    return &Analyzer{ptr: ptr}, nil
}

// NewChineseAnalyzer creates a Chinese analyzer with Jieba
func NewChineseAnalyzer(dictPath string) (*Analyzer, error) {
    cDictPath := C.CString(dictPath)
    defer C.free(unsafe.Pointer(cDictPath))

    ptr := C.diagon_create_chinese_analyzer(cDictPath)
    if ptr == nil {
        return nil, fmt.Errorf("failed to create Chinese analyzer")
    }
    return &Analyzer{ptr: ptr}, nil
}

// Close releases analyzer resources
func (a *Analyzer) Close() error {
    if a.ptr != nil {
        C.diagon_destroy_analyzer(a.ptr)
        a.ptr = nil
    }
    return nil
}

// Analyze analyzes text and returns tokens
func (a *Analyzer) Analyze(text string) ([]string, error) {
    cText := C.CString(text)
    defer C.free(unsafe.Pointer(cText))

    tokens := C.diagon_analyze_text(a.ptr, cText)
    if tokens == nil {
        return nil, fmt.Errorf("analysis failed")
    }
    defer C.diagon_free_tokens(tokens)

    result := make([]string, int(tokens.count))
    tokensSlice := (*[1 << 30]*C.char)(unsafe.Pointer(tokens.tokens))[:tokens.count:tokens.count]

    for i := 0; i < int(tokens.count); i++ {
        result[i] = C.GoString(tokensSlice[i])
    }

    return result, nil
}

// SetFieldAnalyzer sets analyzer for a specific field
func (s *Shard) SetFieldAnalyzer(fieldName string, analyzer *Analyzer) error {
    cFieldName := C.CString(fieldName)
    defer C.free(unsafe.Pointer(cFieldName))

    ret := C.diagon_set_field_analyzer(s.writerPtr, cFieldName, analyzer.ptr)
    if ret != 0 {
        return fmt.Errorf("failed to set field analyzer")
    }
    return nil
}
```

**Tests**: `pkg/data/diagon/analyzer_test.go`

```go
func TestStandardAnalyzer(t *testing.T) {
    analyzer, err := NewStandardAnalyzer()
    require.NoError(t, err)
    defer analyzer.Close()

    tokens, err := analyzer.Analyze("The Quick Brown Fox Jumps!")
    require.NoError(t, err)

    expected := []string{"quick", "brown", "fox", "jumps"}
    assert.Equal(t, expected, tokens)
}

func TestChineseAnalyzer(t *testing.T) {
    analyzer, err := NewChineseAnalyzer("./upstream/dicts/jieba")
    require.NoError(t, err)
    defer analyzer.Close()

    tokens, err := analyzer.Analyze("我爱北京天安门")
    require.NoError(t, err)

    expected := []string{"我", "爱", "北京", "天安门"}
    assert.Equal(t, expected, tokens)
}
```

---

### Phase 8: Index Configuration (1 day)

**Index settings with analyzers**:

```json
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_standard": {
          "type": "standard"
        },
        "my_chinese": {
          "type": "chinese",
          "dict_path": "/path/to/jieba/dicts"
        },
        "my_custom": {
          "type": "custom",
          "tokenizer": "standard",
          "filter": ["lowercase", "stop", "synonym"]
        }
      },
      "filter": {
        "my_synonym": {
          "type": "synonym",
          "synonyms": [
            "laptop, notebook, computer",
            "phone, mobile, smartphone"
          ]
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "title": {
        "type": "text",
        "analyzer": "my_standard"
      },
      "content_en": {
        "type": "text",
        "analyzer": "standard"
      },
      "content_zh": {
        "type": "text",
        "analyzer": "my_chinese"
      },
      "keyword_field": {
        "type": "keyword"
      }
    }
  }
}
```

---

## Testing Strategy

### Unit Tests (Per Component)

1. **Token Tests**
   - Construction
   - Getters/setters
   - Equality

2. **Tokenizer Tests**
   - Each tokenizer individually
   - Edge cases (empty, whitespace, unicode)
   - Performance benchmarks

3. **Filter Tests**
   - Each filter individually
   - Chaining filters
   - Edge cases

4. **Analyzer Tests**
   - Built-in analyzers
   - Custom analyzer composition
   - End-to-end analysis

### Integration Tests

1. **Indexing with Analyzers**
   - Index documents with different analyzers
   - Verify token storage
   - Query indexed documents

2. **Search with Analyzers**
   - Query-time analysis
   - Match results correctly
   - Synonym expansion works

3. **Chinese Analysis**
   - Pure Chinese text
   - Mixed Chinese/English
   - Various Jieba modes

### Performance Tests

1. **Throughput**
   - Tokens/second per analyzer
   - Comparison: Standard vs Chinese vs Multilingual

2. **Memory**
   - Analyzer memory footprint
   - Dictionary loading impact
   - Token buffer sizes

3. **Latency**
   - Per-document analysis time
   - P50, P95, P99 latencies

---

## Dependencies

### Required Libraries

1. **ICU (International Components for Unicode)**
   - Unicode text segmentation
   - Case folding
   - Normalization
   - Installation: `apt-get install libicu-dev`

2. **cppjieba**
   - Chinese word segmentation
   - Header-only library (easy integration)
   - Repository: https://github.com/yanyiwu/cppjieba

3. **Jieba Dictionaries**
   - Download from Jieba repository
   - Bundle with Diagon or load at runtime
   - Size: ~10MB total

### Optional Libraries

1. **Snowball Stemmer** (for stemming)
   - Porter stemmer algorithm
   - Multiple languages

2. **hunspell** (for spell checking)
   - Dictionary-based spell checker

---

## Timeline

### Detailed Schedule (10-12 days)

| Day | Phase | Tasks | Hours |
|-----|-------|-------|-------|
| 1-2 | Phase 1 | Core framework (Token, Tokenizer, Filter, Analyzer) | 16h |
| 3-4 | Phase 2 | Standard tokenizers (Whitespace, Keyword, Standard) | 16h |
| 5-7 | Phase 3 | Jieba integration (library, tokenizer, testing) | 24h |
| 8-9 | Phase 4 | Token filters (Lowercase, Stop, ASCII, Synonym) | 16h |
| 10 | Phase 5 | Built-in analyzers (Standard, Chinese, Multilingual) | 8h |
| 11 | Phase 6 | C API exposure | 8h |
| 12 | Phase 7 | Go integration | 16h |
| 13 | Phase 8 | Index configuration | 8h |

**Total**: ~112 hours (14 days with 8h/day)

---

## Success Criteria

### Functional Requirements

- ✅ Standard tokenization works for English text
- ✅ Chinese tokenization works via Jieba
- ✅ Token filters process streams correctly
- ✅ Analyzers compose tokenizers and filters
- ✅ C API exposes all functionality
- ✅ Go bridge integrates seamlessly
- ✅ Index settings configure analyzers
- ✅ Query-time analysis works

### Performance Requirements

- Throughput: >10,000 docs/sec with standard analyzer
- Throughput: >5,000 docs/sec with Chinese analyzer
- Latency: <10ms per document (standard analyzer)
- Latency: <20ms per document (Chinese analyzer)
- Memory: <50MB for analyzer instances + dictionaries

### Quality Requirements

- Unit test coverage: >90%
- Integration test coverage: >80%
- No memory leaks (valgrind clean)
- Thread-safe analyzer instances
- Proper error handling and reporting

---

## Migration Path

### Phase 1: Basic Support (Days 1-8)
- Core framework + Standard analyzers
- No Chinese support yet
- Limited to English

### Phase 2: Chinese Support (Days 9-12)
- Jieba integration
- Chinese analyzer
- Multilingual support

### Phase 3: Advanced Features (Future)
- Custom analyzer plugins
- More token filters (stemming, n-grams)
- Phonetic analysis
- More languages (Japanese, Korean, Arabic)

---

## Documentation

### User Documentation

1. **Analyzer Configuration Guide**
   - How to configure analyzers
   - Built-in analyzer reference
   - Custom analyzer examples

2. **Chinese Analysis Guide**
   - Jieba mode selection
   - Dictionary customization
   - Performance tuning

3. **API Reference**
   - C API documentation
   - Go API documentation
   - Code examples

### Developer Documentation

1. **Architecture Overview**
   - Component diagram
   - Data flow
   - Extension points

2. **Adding New Tokenizers**
   - Interface implementation
   - Testing guidelines
   - Best practices

3. **Performance Tuning**
   - Profiling tools
   - Optimization techniques
   - Benchmarking

---

## Risks and Mitigations

### Risk 1: Jieba Integration Complexity

**Risk**: cppjieba may be hard to integrate or have compatibility issues

**Mitigation**:
- Test integration early (Day 1-2)
- Have fallback plan: simpler Chinese tokenizer
- Consider alternative: icu::BreakIterator with Chinese rules

### Risk 2: Performance Impact

**Risk**: Analysis adds significant indexing overhead

**Mitigation**:
- Benchmark early and often
- Optimize hot paths (token creation, filter chains)
- Consider token object pooling
- Profile with real data

### Risk 3: Memory Usage

**Risk**: Jieba dictionaries and analyzer instances use too much memory

**Mitigation**:
- Lazy load dictionaries
- Share analyzer instances across threads
- Implement dictionary unloading for idle analyzers
- Monitor memory usage in production

### Risk 4: Unicode Complexity

**Risk**: Unicode handling is error-prone

**Mitigation**:
- Use ICU library (battle-tested)
- Comprehensive test cases
- Test with various languages
- Handle edge cases (emoji, RTL text, etc.)

---

## Future Enhancements

### Short Term (3-6 months)

1. **More Token Filters**
   - Stemming (Porter, Snowball)
   - N-gram tokenizer (for autocomplete)
   - Edge N-gram filter
   - Phonetic filters (Soundex, Metaphone)

2. **More Languages**
   - Japanese (Kuromoji)
   - Korean (Nori)
   - Arabic
   - Thai

3. **Analysis Debugging**
   - Analyze API endpoint
   - Token visualization
   - Performance profiling

### Long Term (6-12 months)

1. **ML-Based Tokenization**
   - BERT tokenizer
   - SentencePiece
   - Custom models

2. **Advanced Features**
   - Part-of-speech tagging
   - Named entity recognition
   - Semantic analysis

3. **Plugin System**
   - Custom analyzer plugins
   - User-provided tokenizers
   - Dynamic loading

---

## Conclusion

This analyzer framework provides:

✅ **Solid Foundation**: Extensible architecture for text analysis
✅ **Chinese Support**: Production-ready Jieba integration
✅ **Performance**: Optimized for high-throughput indexing
✅ **Quality**: Comprehensive testing and error handling
✅ **Integration**: Clean C API and Go bridge

**Recommended Approach**: Implement in phases, starting with Phase 1-2 (core + standard tokenizers), then add Chinese support (Phase 3) once the foundation is stable.

**Estimated Total Time**: 14 days (112 hours)
**Priority**: HIGH (foundation for search quality)
**Complexity**: MEDIUM-HIGH (but manageable with phased approach)

---

**Created**: January 27, 2026
**Author**: Quidditch Team
**Status**: DESIGN COMPLETE - Ready for Implementation
